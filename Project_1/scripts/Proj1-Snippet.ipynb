{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code for COSC522 Project 1 - How to modularize your code\n",
    "This code shows another extreme to Proj1-Case1, where a new class \"mpp\" is defined that includes a training method (fit) and a testing method (predict). It also defines one function to load data (load_data), and a function to evaluate the data (accuracy_score). The latter is not complete so you need to try to fill in the blanks. Both the class and the functions can be reused by other projects or when you use another dataset.\n",
    "\n",
    "If you go through the main function first, the flow should be very clear.\n",
    "Step 1: read in the datasets\n",
    "Step 2: train the model using the training set\n",
    "Step 3: test the model using the test set\n",
    "Step 4: evaluate the performance of the model\n",
    "\n",
    "In this implementation, I also tried to introduce a new structure, \"dictionary\", instead of using \"array\" for the covariance matrices and means of different categories. I'll leave it to you to figure out what is a \"dictionary\" and what is the benefit of using it.\n",
    "\n",
    "I'd like to draw your attention of the difference between the native Python array and a numpy array. In this implementation, I made an effort of always using a numpy array for its rich features although this means a little deficiency in efficiency.\n",
    "\n",
    "Finally, if this is difficult for you to digest, you can always use Proj1-Case1 but I'd strongly urge you to start from a good programming structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(f):\n",
    "    \"\"\" Assume data format:\n",
    "    feature1 feature 2 ... label \n",
    "    \"\"\"\n",
    "\n",
    "    # process training data\n",
    "    data = np.genfromtxt(f)\n",
    "    # return all feature columns except last\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1].astype(int)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y, y_model):\n",
    "    \"\"\" Return accuracy score.\n",
    "    You are supposed to return both overall accuracy and classwise accuracy.\n",
    "    The following code only returns overall accuracy\n",
    "    \"\"\"\n",
    "    assert len(y) == len(y_model)\n",
    "\n",
    "    classn = len(np.unique(y))    # number of different classes\n",
    "    correct_all = y == y_model    # all correctly classified samples\n",
    "\n",
    "    acc_overall = np.sum(correct_all) / len(y)\n",
    "    acc_i = []        # this list7 stores classwise accuracy\n",
    "    \n",
    "    \"\"\"calculate classwise accuracy\n",
    "    you need to fill in this part\n",
    "    \"\"\"\n",
    "\n",
    "    return acc_i, acc_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mpp:\n",
    "    \"\"\"Maximum Posterior Probability\n",
    "    Supervised parametric learning assuming Gaussian pdf\n",
    "    with 3 cases of discriminant functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, case, prior):\n",
    "        self.case_ = case\n",
    "        self.prior_ = prior        \n",
    "        \n",
    "    def fit(self, Tr, y):\n",
    "        # derive the model \n",
    "        self.covs_, self.means_, self.pw_ = {}, {}, {}     # dictionaries\n",
    "        self.covsum_ = None\n",
    "\n",
    "        self.classes_ = np.unique(y)     # get unique labels as dictionary items\n",
    "        self.classn_ = len(self.classes_) # the number of classes in the dataset\n",
    "\n",
    "        assert self.classn_ == len(self.prior_)  \n",
    "        k = 0       # to convert the prior probability from array to dictionary\n",
    "        for c in self.classes_:\n",
    "            arr = Tr[y == c]\n",
    "            self.covs_[c] = np.cov(np.transpose(arr))\n",
    "            self.means_[c] = np.mean(arr, axis=0)  # mean along rows\n",
    "            self.pw_[c] = self.prior_[k]\n",
    "            k = k + 1\n",
    "            if self.covsum_ is None:\n",
    "                self.covsum_ = self.covs_[c]\n",
    "            else:\n",
    "                self.covsum_ += self.covs_[c]\n",
    "\n",
    "        # used by case II\n",
    "        self.covavg_ = self.covsum_ / self.classn_\n",
    "\n",
    "        # used by case I\n",
    "        self.varavg_ = np.sum(np.diagonal(self.covavg_)) / self.classn_\n",
    "\n",
    "    def predict(self, Te):\n",
    "        # predict labels of all test data \n",
    "        y = []      # list to hold the predicted label\n",
    "        disc = np.zeros(self.classn_)\n",
    "        nr, _ = Te.shape\n",
    "\n",
    "        for i in range(nr):         # going through each sample (or each row of the test set)\n",
    "            for c in self.classes_:  # going through each class or category\n",
    "                if self.case_ == 1:\n",
    "                    edist2 = np.dot(Te[i]-self.means_[c], Te[i]-self.means_[c])\n",
    "                    disc[c] = -edist2 / (2 * self.varavg_) + np.log(self.pw_[c])\n",
    "                elif self.case_ == 2: \n",
    "                    \"implement minimum Mahalanobis classifier\"\n",
    "                elif self.case_ == 3:\n",
    "                    \"implement quadratic machine\"\n",
    "                else:\n",
    "                    print(\"Can only handle case numbers 1, 2, 3.\")\n",
    "                    sys.exit(1)\n",
    "            y.append(disc.argmax())\n",
    "            \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of the training data is: (250, 2)\n",
      "The dimension of the testing data is: (1000, 2)\n",
      "The overall classification accuracy is 0.713\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # read in the datasets\n",
    "    Xtrain, ytrain = load_data('synth.tr')\n",
    "    Xtest, ytest = load_data('synth.te')\n",
    "    print(f\"The dimension of the training data is: {Xtrain.shape}\")\n",
    "    print(f\"The dimension of the testing data is: {Xtest.shape}\")\n",
    "\n",
    "    # specify the prior probability, and the cases\n",
    "    prior = np.array([0.5,0.5])\n",
    "    case = 1\n",
    "    \n",
    "    # create an object of the model initialized by the case # and prior probability\n",
    "    model = mpp(case, prior)\n",
    "    # train the model using the training set\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    # test the model using the test set\n",
    "    y_model = model.predict(Xtest)\n",
    "    # evaluate the performance of the model\n",
    "    acc_classwise, acc_overall = accuracy_score(ytest, y_model)\n",
    "    print(f\"The overall classification accuracy is {acc_overall}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
